---
title: "CSE 120 - Computer Architecture Notes (In Progress)"
excerpt_separator: "<!--more-->"
classes: "wide"
categories:
  - Notes

tags:
  - computer architecture
  - 
---

These are my notes from CSE120 Computer Architecture, taught by Prof. Nath in Winter 2022 quarter. It is based on this [book](http://home.ustc.edu.cn/~louwenqi/reference_books_tools/Computer%20Organization%20and%20Design%20RISC-V%20edition.pdf).

## Performance

**Moore's Law** is the observation that the number of transistors per chip in an economical IC doubles approximately every *18-24 months*.

**Dennard Scaling(1974)** $\to$ observation that voltage and current *should* be proportional to the linear dimensions of a transistor. As transistors shrank, so did the necessary voltage and curent because power is proportional to the area of the transistor.
- A way of scaling transistor parameters (including voltage) to keep power density constant
- Ignored *leakage current* (when a transistor is sitting idle, how much current is it leaking) and *threshold voltage* (minimum amount of voltage needed to turn a transistor on and off, doesn't scale well past below 65nm), which caused the end of dennard scaling, which has led to flatline in clock speed of CPUs @ ~4.5 GHz.

CPUs haven't improved much at single core performance, most gains come from having multiple cores, parallelism, speculative prediction, etc, all of which give a performance boost beyond transistor constraints.
  

**Dynamic Power** dissipation of $\alpha * C * f * V^2$ where
- $\alpha$ = percent time switch
- $C$  = capacitance
- $f$ = frequency
- $V$ = Voltage  
As the size of the transistors shrunk, voltage was reduced which allowed circuits to operate at higher frequencies at the same power.(Intuitively, if both $C$ and $V$ decrease, we can increase $f$)

<p align="center">
  <img src="/images/cse120/dennard_scaling.png" width = "100%">
</p>

**Latency** $\to$ interval between stimulation and response (execution time)  
**Throughput** $\to$ total work done per unit of time (e.g. queries/sec). Throughput = $\frac{1}{Latency}$ when we can't do tasks in parallel

**clock period**  $\to$ duration of a clock cycle (basic unit of time for computers)  
**clock frequency** $\to$ $\frac{1}{T_p}$ where $T_p$ is the time for one clock period in seconds.

**Execution time**  = $\frac{C_{pp} * C_{ct}}{C_r}$, $C_{pp}$ = Cycles per program, $C_{ct}$ = Clock cycle time, ${C_r}$ = clock rate
  - execution time by either increasing clock rate or decreasing the number of clock cycles.

**Performance** For a machine $A$ running a program $P$ (where higher is faster):
$Perf(A,P) = \frac{1}{Time(A,P)}$  
$Perf(A,P) > Perf(B,P) \to Time(A,P) < Time(B, P)$  
$\frac{Perf(A,P)}{Perf(B,P)} = \frac{Time(B,P)}{Time(A,P)} = n$, where $A$ is $n$ times faster than B when $n > 1$.  
$Speedup = \frac{Time(old)}{Time(new)}$

**Little's Law** $\to Parellelism = Throughput * Latency$

- CPU TIME $\to$ the actual time the CPU spends computing for a specific task.
  - *user cpu time* $\to$ the CPU time spent in a program itself
  - *system CPU time* $\to$ The CPU time spent in the operating system performing tasks on
behalf of the program.

**Clock cycles per instructions(CPI)** $\to$ is the average number of clock cycles each instruction takes to execute. We use *CPI* as an average of all the instructions executed in a program, which accounts for different instructions taking different amounts of time.
  - determined by hardware design, different instructions $\to$ different CPI 

$CPU\ Time = I_c * CPI * C_{ct}$ where $I_c = $ instruction count and $C_{ct} =$ clock cycle time.
$CPU\ Time = \frac{I_c * CPI}{C_r}$ where $C_r$ = clock rate. Clock rate is the inverse of clock cycle time.

Measuring performance of a CPU requires us to know the number of instrutions, the clock cycles per instruction, and the clock cycle time. We can measure instruction count by using software tools that profile the execution, or we can use hardware counters which can record the number of instructions executed. Instruction count depends on the architecture, but not the exact implementation. CPI is much more difficult to measure, because it relies on a wide variety of design details in the computer (like the memory and processor structure), as well as the mix of different instruction types executed in an application. As a result, CPI varies by application, as well as implementations of with the same instruction set. 
  - Using time as a performative metric is often misleading, and a better alternative is **MIPS (million instructions per second)** $\to \ \frac{I_c}{Exec_{time} * 10^6}$  
    - 3 problems with MIPS when comparing MIPS between computers
      - can't compare computers with different instruction sets, because each instruction has varying amounts of capability
      - MIPS varies on the same computer depending on the program being run, which means there is no universal MIPS rating for a computer
      - Lastly, if a computer executes more instructions, and each instruction is faster, than MIPS can vary independently from performance.

Given $n$ processors, $Speedup_n = \frac{T_1}{T_n}$, $T_1 > 1$ is the execution time one *one core*, $T_n$ is the execution time on $n$ cores.  
  - $Speedup\ efficiency_n \to Efficiency_n = \frac{Speedup_n}{n}$

**Amdahl's Law** $\to$ a harsh reality for parallel computing.
  - $Speedup_n = \frac{T_1}{T_n} = \frac{1}{\frac{F_{parallel}}{n} + F_{sequential}} = \frac{1}{\frac{F_{parallel}}{n} +\ (1-F_{parallel})} $
  - using $n$ cores will result in a speedup of $n$ times over 1 core $\to$ **FALSE**
  - states that some fraction of total operation is inherently sequential and impossible to parallelize (like reading data, setting up calculations, control logic, and storing results). Think sequential operation like RNNs and LSTMs.
<p align="center">
  <img src="/images/cse120/amdahl_law_visual.png" width = "80%">
</p>

  - We can save energy and power by make our machines more effiecient at computation $\to$ if we finish the computation faster (even if it takes more energy), the speed up in computation would offset the extra energy use by idling longer and using less energy.
  

**Iron Law** $\to$ $Exec_{time} = \frac{I}{program} * \frac{C_{cycle}}{I} * \frac{secs}{C_{cycle}} = I_c * CPI * C_{ct}$
  - High performance (where execution time is decreased) relies on:
    - *algorithm* $\to$ affects $I_{c}$ and possibly $CPI$
    - *Programming Language* $\to$ affects $I_{c}$ and $CPI$
    - *compiler* $\to$ affects $I_{c}$ and $CPI$
    - *ISA* $\to$ affects $I_{c}$ and $CPI$
    - *hardware design* $\to$ affects $CPI$ and $C_{ct}$

The **Instruction set architecture (ISA)** is an abstraction layer $\to$ is the part of the processor that is visible to the programmer or compiler writer.
  - ISA operates on the CPU and memory to produce desired output from instructions
  - this allows ISA abstraction for different layers, which allows *different* transistor types can be used to implement the *same ISA*.
  - Acts as a *hardware-software interface*:
    - it defines:
      - the *state of the system* (registers, memory) $\to$ what values do they contain, are they stack pointers
      - *functionality of each instruction* $\to$ ```add``` ```sub``` ```load``` , etc
      - *encoding of each HW instruction* $\to$ how are instructions converted to bit sequences?
    - it doesn't define:
      - how instructions are implemented in the underlying hardware
      - how fast/slow instructions are
      - how much power instructions consume
  - **Big Idea** $\to$ a good ISA separates *architecture* (what) from *implmentation* (how).

<p align="center">
  <img src="/images/cse120/ISA_diagram.png" width = "70%">
</p>


Computers only work with *bits* (0s and 1s)
  - we express complex things like numbers, pictures, and strings as a sequence of bits
  - memory cells preserve bits over time $\to$ flip-flops, registers, SRAM, DRAM
  - logic gates operate on bits (AND, OR, NOT, multiplexor)

In order to get hardware to compute something, we express the task as a sequence of bits. **Abstraction** is a key concept that allows us to build large, complex programs, that would be impossible in just binary. **Machine language**, which is simply binary instructions are what computers understand, but programming in binary is extremely slow and difficult. To circumvent this, we have **assembly language**, which takes an instruction such as ```add A, B ``` and passes it through an *assembler*, which simply translate a symbolic version of instructions into the binary version. While this is an improvement over binary in readability and easibility of coding, it is still inefficient, since a programmer needs to write one line for each instruction that the computer will follow. This brings us to **compilers**, which *compile* a high level language into instructions that the computer can understand (high level language $\to$ assembly language), which allow us to write out more complex tasks in fewer lines of code.

## RISC-V

**RISC-V** (RISC $\to$ Reduced Instruction Set Computer)is an open-source ISA developed by UC Berkeley, which is built on the philosphy that simple and small ISA allow for simple and fast hardware. RISC-V is little-endian.
  - Note: **CISC (Complex Instruction Set Computer)** $\to$  get more done by adding more sophistacated instructions (each instruction does more things, which reduces the number of instructions required for a program, but increases the *CPI*)
    - ```MUL mem2 <= mem0 * mem1``` $\to$ helps save memory by writing fewer lines of code (very important during 80s, not so much anymore), downside is that hardware logic becomes much more complicated
  - Internally, Intel/AMD are CISC instructions get dividing into **$\mu$ ops** (micro-ops), where one CISC operation splits into multiple micro-ops.
    -  smaller code footprint of CISC and processor simplicity of RISC
  
Instruction Cycle Summary:
  - PC **fetches** instruction from memory
  - Processor **decodes** the instruction (is it an add, store, branch, etc)
  - Logically **execute** the instruction (logically compute x1 + x2)
  - **Access memory** to write in result of computation
  - **Write back** to register (x3 now contains x1 + x2)
    
  
Basic RISC-V syntax:
  - ```addi dst, src1, immediate``` $\to$ we add ```src1```, with an ```immediate``` value (aka a constant 12 big signed value) and store it in ```dst```
    - ```addi x1, x1, 3``` $\to  a = a + 3$


RISC-V follows the following design principles:
  - *Design Principle 1*: Simplicity favors regularity
  - *Design Principle 2*: Smaller is faster
  - *Design Principle 3*: Good design demands good compromises.

### Simplicity favors regularity
RISC-V notation is rigid: each RISC-V arithmetic instrution only performs one operation and requires three variables. Each line of RISC-V can only contain one instruction.
- ```add a, b, c ``` $\to$ instructs a computer to add two variables ```b``` and ```c``` and store the result in ```a```.
- adding variables ```b``` + ```c``` + ```d``` and storing into ```a```
  - ```add a, b, c``` $\to$ The sum of b and c is placed in a
  - ```add a, a, d``` $\to$ The sum of b, c, and d is now in a
  - ```add a, a, e``` $\to$ The sum of b, c, d, and e is now in a

### Smaller is faster
Arithmetic operations take place on **registers** $\to$ primitives used in hardware design that are visible to the programmer when the computer is completed. Register sizes in RISC-V are 64 bits (doublewords) and instructions are 32 bits. There are typically around 32 registers found on current computers, because more registers increases the clock cycle time since electrical signals have to travel further.  

Since registers have a very small limited amount of data, we keep larger things, like data structures, in memory. When we want to perform operations on our data structures, we transfer the data from the memory to the registers, which is called **data structure instructions**. We use a *load* operation ```ld``` to load an object in memory into a register. *store* is the complement of the *load* operation, where ```sd``` allows us to copy data from a register to memory.
  - ```ld dst, offset(base)```
    - ```ld x1, 15(x5)``` store the contents of address 8015 in register ```x1``` where ```x5``` = 8000 and ```15``` is our offset.

Most programs today have more variables than registers, which requires compilers to keep the most frequently used variables in registers and place the remaining variables in memory (latter is called **spilling**). Data in registers is much more useful, because we can read two registers, operate on them, and write the result. Data in memory requires two separate operands to *load* and *store* the memory, without operating on it. Data in registers take less time to access and have a higher throughput than memory, and use less energy than accessing memory. 


## The Processor

## Pipelining
**Pipelining** $\to$ implementation technique in which multiple instructions are overlapped in execution (like an assembly line). We can't improve *latency* but we can improve *throughput*.
  - *Analogy:* If we have two loads of laundry, we put the first load in the washer. When it's done, we put it in the dryer,and we place the second load in the washer. When both loads are done we take the first load out of the dryer and we put the second load into the dryer. 
    - built on the idea that as long as we have separate resources for each stage, we can pipeline the tasks. Each step is considered a *stage* in our pipeline.

RISC-V is highly optimized for pipelining because each instruction is the same lenght (32 bits). RISC-V also has fewer instruction formats, where source and destination registers are located in the same place for each instruction. Lastly, the only memory operands are load and store, which makes shorter pipelines.

<p align="center">
  <img src="/images/cse120/pipelining_schedule.png" width = "90%">
</p>

Here we can see an example of a pipelining process. We can see a large difference between pipelined process and non-pipelined process below.
<p align="center">
  <img src="/images/cse120/pipeline_vs_non.png" width = "90%">
</p>

## Hazards
**Structural Hazard** $\to$ when a planned instruction cannot execute in the proper clock cycle because the hardware doesn't support the combinations of instructions that are set to execute. 
  - Ex: If we go back to the earlier pipeline stage, if we had a single memory instead of two memories, our first instruction access data from memory, while our fourth instruction is fetching an instruction from the same memory.  

**Data Hazard** $\to$ when a pipeline is stalled because one pipeline must wait for another pipeline to finish.
  -  ```add x19, x0, x1```  
```sub x2, x19, x3```  
    - Our add instruction doesn't write until the fifth stage, so we have to wait three clock cycles to save our value. We have a dependence of one instruction on an earlier one that is still in the pipeline.

  **Forwarding (bypassing)** $\to$ is the process of retrieving the missing data elements from internal buffers rather than waiting for it to arrive to the registers or the memory.

**Control Hazards (aka branch hazard)** $\to$  when the proper instruction cannot execute in the proper pipeline clock cycle because the instruction that was fetched is not the one that is needed; that is, the flow of instruction addresses is not what the pipeline expected.
        

## Cache
In order to speed up memory access, we employ the *principle of locality*, where programs only need to access a relatively small portion of address space.
  - **Temporal locality (time locality)** $\to$ items that are referenced will probably need to be referenced soon.
  - **Spatial locality (space locality)** $\to$ items that are referenced, its neighbors (of addresses) will probably be accessed soon.
  - **Memory hierarchy** $\to$ multiple levels of memory with different speeds and sizes. Faster memory is more expensive and thus smaller.

Main memory is implemented in DRAM (dynamic random access memory), where levels closer to the processor (caches) use SRAM (static random access memory).
  - *Cache:* $\to$ small amount of fast, expensive memory
    - *L1 cache:* usually on CPU chip, 2-4x slower than CPU mem 
    - *L2:* $\to$ maybe on or off chip
    - *L3:* $\to$ off-chip made of SRAM
  - *main memory* $\to$ medium price, medium speed (DRAM)
  - *disk* $\to$ many TBs of non-volatile, slow, cheap memory


## Virtual Memory


## stack vs heap