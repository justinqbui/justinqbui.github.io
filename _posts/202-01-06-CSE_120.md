---
title: "CSE 120 - Computer Architecture Notes (In Progress)"
excerpt_separator: "<!--more-->"
classes: "wide"
categories:
  - Notes

tags:
  - computer architecture
  - 
---

These are my notes from CSE120 Computer Architecture, taught by Prof. Nath in Winter 2022 quarter. It is based on this [book](http://home.ustc.edu.cn/~louwenqi/reference_books_tools/Computer%20Organization%20and%20Design%20RISC-V%20edition.pdf).

**Moore's Law** is the observation that the number of transistors per chip in an economical IC doubles approximately every *18-24 months*.

**Dennard Scaling(1974)** $\to$ observation that voltage and current *should* be proportional to the linear dimensions of a transistor. As transistors shrank, so did the necessary voltage and curent because power is proportional to the area of the transistor.
- A way of scaling transistor parameters (including voltage) to keep power density constant
- Ignored *leakage current* (when a transistor is sitting idle, how much current is it leaking) and *threshold voltage* (minimum amount of voltage needed to turn a transistor on and off, doesn't scale well past below 65nm), which caused the end of dennard scaling, which has led to flatline in clock speed of CPUs @ ~4.5 GHz.

CPUs haven't improved much at single core performance, most gains come from having multiple cores, parallelism, speculative prediction, etc, all of which give a performance boost beyond transistor constraints.
  

**Dynamic Power** dissipation of $\alpha * C * f * V^2$ where
- $\alpha$ = percent time switch
- $C$  = capacitance
- $f$ = frequency
- $V$ = Voltage  
As the size of the transistors shrunk, voltage was reduced which allowed circuits to operate at higher frequencies at the same power.(Intuitively, if both $C$ and $V$ decrease, we can increase $f$)

<p align="center">
  <img src="/images/cse120/dennard_scaling.png" width = "100%">
</p>

**Latency** $\to$ interval between stimulation and response (execution time)  
**Throughput** $\to$ total work done per unit of time (e.g. queries/sec). Throughput = $\frac{1}{Latency}$ when we can't do tasks in parallel

**clock period**  $\to$ duration of a clock cycle (basic unit of time for computers)  
**clock frequency** $\to$ $\frac{1}{T_p}$ where $T_p$ is the time for one clock period in seconds.

**Execution time**  = $\frac{C_{pp} * C_{ct}}{C_r}$, $C_{pp}$ = Cycles per program, $C_{ct}$ = Clock cycle time, ${C_r}$ = clock rate
  - execution time by either increasing clock rate or decreasing the number of clock cycles.

**Performance** For a machine $A$ running a program $P$ (where higher is faster):
$Perf(A,P) = \frac{1}{Time(A,P)}$  
$Perf(A,P) > Perf(B,P) \to Time(A,P) < Time(B, P)$  
$\frac{Perf(A,P)}{Perf(B,P)} = \frac{Time(B,P)}{Time(A,P)} = n$, where $A$ is $n$ times faster than B when $n > 1$.  
$Speedup = \frac{Time(old)}{Time(new)}$

**Little's Law** $\to Parellelism = Throughput * Latency$

- CPU TIME $\to$ the actual time the CPU spends computing for a specific task.
  - *user cpu time* $\to$ the CPU time spent in a program itself
  - *system CPU time* $\to$ The CPU time spent in the operating system performing tasks on
behalf of the program.

**Clock cycles per instructions(CPI)** $\to$ is the average number of clock cycles each instruction takes to execute. We use *CPI* as an average of all the instructions executed in a program, which accounts for different instructions taking different amounts of time.
  - determined by hardware design, different instructions $\to$ different CPI 

$CPU\ Time = I_c * CPI * C_{ct}$ where $I_c = $ instruction count and $C_{ct} =$ clock cycle time.
$CPU\ Time = \frac{I_c * CPI}{C_r}$ where $C_r$ = clock rate. Clock rate is the inverse of clock cycle time.

Measuring performance of a CPU requires us to know the number of instrutions, the clock cycles per instruction, and the clock cycle time. We can measure instruction count by using software tools that profile the execution, or we can use hardware counters which can record the number of instructions executed. Instruction count depends on the architecture, but not the exact implementation. CPI is much more difficult to measure, because it relies on a wide variety of design details in the computer (like the memory and processor structure), as well as the mix of different instruction types executed in an application. As a result, CPI varies by application, as well as implementations of with the same instruction set. 
  - Using time as a performative metric is often misleading, and a better alternative is **MIPS (million instructions per second)** $\to \ \frac{I_c}{Exec_{time} * 10^6}$  
    - 3 problems with MIPS when comparing MIPS between computers
      - can't compare computers with different instruction sets, because each instruction has varying amounts of capability
      - MIPS varies on the same computer depending on the program being run, which means there is no universal MIPS rating for a computer
      - Lastly, if a computer executes more instructions, and each instruction is faster, than MIPS can vary independently from performance.

Given $n$ processors, $Speedup_n = \frac{T_1}{T_n}$, $T_1 > 1$ is the execution time one *one core*, $T_n$ is the execution time on $n$ cores.  
  - $Speedup\ efficiency_n \to Efficiency_n = \frac{Speedup_n}{n}$

**Amdahl's Law** $\to$ a harsh reality for parallel computing.
  - $Speedup_n = \frac{T_1}{T_n} = \frac{1}{\frac{F_{parallel}}{n} + F_{sequential}} = \frac{1}{\frac{F_{parallel}}{n} +\ (1-F_{parallel})} $
  - using $n$ cores will result in a speedup of $n$ times over 1 core $\to$ **FALSE**
  - states that some fraction of total operation is inherently sequential and impossible to parallelize (like reading data, setting up calculations, control logic, and storing results). Think sequential operation like RNNs and LSTMs.
<p align="center">
  <img src="/images/cse120/amdahl_law_visual.png" width = "80%">
</p>

  - We can save energy and power by make our machines more effiecient at computation $\to$ if we finish the computation faster (even if it takes more energy), the speed up in computation would offset the extra energy use by idling longer and using less energy.
  

**Iron Law** $\to$ $Exec_{time} = \frac{I}{program} * \frac{C_{cycle}}{I} * \frac{secs}{C_{cycle}} = I_c * CPI * C_{ct}$
  - High performance (where execution time is decreased) relies on:
    - *algorithm* $\to$ affects $I_{c}$ and possibly $CPI$
    - *Programming Language* $\to$ affects $I_{c}$ and $CPI$
    - *compiler* $\to$ affects $I_{c}$ and $CPI$
    - *ISA* $\to$ affects $I_{c}$ and $CPI$
    - *hardware design* $\to$ affects $CPI$ and $C_{ct}$

The **Instruction set architecture (ISA)** is an abstraction layer $\to$ is the part of the processor that is visible to the programmer or compiler writer.
  - ISA operates on the CPU and memory to produce desired output from instructions
  - this allows ISA abstraction for different layers, which allows *different* transistor types can be used to implement the *same ISA*.
  - Acts as a *hardware-software interface*:
    - it defines:
      - the *state of the system* (registers, memory) $\to$ what values do they contain, are they stack pointers
      - *functionality of each instruction* $\to$ ```add``` ```sub``` ```load``` , etc
      - *encoding of each HW instruction* $\to$ how are instructions converted to bit sequences?
    - it doesn't define:
      - how instructions are implemented in the underlying hardware
      - how fast/slow instructions are
      - how much power instructions consume
  - **Big Idea** $\to$ a good ISA separates *architecture* (what) from *implmentation* (how).

<p align="center">
  <img src="/images/cse120/ISA_diagram.png" width = "70%">
</p>


Computers only work with *bits* (0s and 1s)
  - we express complex things like numbers, pictures, and strings as a sequence of bits
  - memory cells preserve bits over time $\to$ flip-flops, registers, SRAM, DRAM
  - logic gates operate on bits (AND, OR, NOT, multiplexor)

In order to get hardware to compute something, we express the task as a sequence of bits. **Abstraction** is a key concept that allows us to build large, complex programs, that would be impossible in just binary. **Machine language**, which is simply binary instructions are what computers understand, but programming in binary is extremely slow and difficult. To circumvent this, we have **assembly language**, which takes an instruction such as ```add A, B ``` and passes it through an *assembler*, which simply translate a symbolic version of instructions into the binary version. While this is an improvement over binary in readability and easibility of coding, it is still inefficient, since a programmer needs to write one line for each instruction that the computer will follow. This brings us to **compilers**, which *compile* a high level language into instructions that the computer can understand (high level language $\to$ assembly language), which allow us to write out more complex tasks in fewer lines of code.

## RISC-V
**RISC-V** (RISC $\to$ Reduced Instruction Set Computer)is an open-source ISA developed by UC Berkeley, which is built on the philosphy that simple and small ISA allow for simple and fast hardware. RISC-V is little-endian.

RISC-V follows the following design principles:
  - *Design Principle 1*: Simplicity favors regularity
  - *Design Principle 2*: Smaller is faster
  - *Design Principle 3*: Good design demands good compromises.

### Simplicity favors regularity
RISC-V notation is rigid: each RISC-V arithmetic instrution only performs one operation and requires three variables. Each line of RISC-V can only contain one instruction.
- ```add a, b, c ``` $\to$ instructs a computer to add two variables ```b``` and ```c``` and store the result in ```a```.
- adding variables ```b``` + ```c``` + ```d``` and storing into ```a```
  - ```add a, b, c``` $\to$ The sum of b and c is placed in a
  - ```add a, a, d``` $\to$ The sum of b, c, and d is now in a
  - ```add a, a, e``` $\to$ The sum of b, c, d, and e is now in a

### Smaller is faster
Arithmetic operations take place on **registers** $\to$ primitives used in hardware design that are visible to the programmer when the computer is completed. Register sizes in RISC-V are 64 bits (doublewords). There are typically around 32 registers found on current computers, because more registers increases the clock cycle time since electrical signals have to travel further.  

Since registers have a very small limited amount of data, we keep larger things, like data structures, in memory. When we want to perform operations on our data structures, we transfer the data from the memory to the registers, which is called **data structure instructions**. We use a *load* operation ```ld``` to load an object in memory into a register. *store* is the complement of the *load* operation, where ```sd``` allows us to copy data from a register to memory.

Most programs today have more variables than registers, which requires compilers to keep the most frequently used variables in registers and place the remaining variables in memory (latter is called **splilling**). Data in registers is much more useful, because we can read two registers, operate on them, and write the result. Data in memory requires two separate operands to *load* and *store* the memory, without operating on it. Data in registers take less time to access and have a higher throughput than memory, and use less energy than accessing memory. 
