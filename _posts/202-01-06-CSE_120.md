---
title: "CSE 120 - Computer Architecture Notes (In Progress)"
excerpt_separator: "<!--more-->"
classes: "wide"
categories:
  - Notes

tags:
  - computer architecture
  - 
---

**Moore's Law** is the observation that the number of transistors per chip in an economical IC doubles approximately every *18-24 months*.

**Dennard Scaling(1974)** $\to$ observation that voltage and current *should* be proportional to the linear dimensions of a transistor. As transistors shrank, so did the necessary voltage and curent because power is proportional to the area of the transistor.
- A way of scaling transistor parameters (including voltage) to keep power density constant
- Ignored *leakage current* (when a transistor is sitting idle, how much current is it leaking) and *threshold voltage* (minimum amount of voltage needed to turn a transistor on and off, doesn't scale well past below 65nm), which caused the end of dennard scaling, which has led to flatline in clock speed of CPUs @ ~4.5 GHz.

CPUs haven't improved much at single core performance, most gains come from having multiple cores, parallelism, speculative prediction, etc, all of which give a performance boost beyond transistor constraints.
  

**Dynamic Power** dissipation of $\alpha * C * f * V^2$ where
- $\alpha$ = percent time switch
- $C$  = capacitance
- $f$ = frequency
- $V$ = Voltage  
As the size of the transistors shrunk, voltage was reduced which allowed circuits to operate at higher frequencies at the same power.(Intuitively, if both $C$ and $V$ decrease, we can increase $f$)

<p align="center">
  <img src="/images/cse120/dennard_scaling.png" width = "100%">
</p>

**Latency** $\to$ interval between stimulation and response (execution time)  
**Throughput** $\to$ total work done per unit of time (e.g. queries/sec). Throughput = $\frac{1}{Latency}$ when we can't do tasks in parallel

**clock period**  $\to$ duration of a clock cycle (basic unit of time for computers)  
**clock frequency** -> $\frac{1}{T_p}$ where $T_p$ is the time for one clock period in seconds.

**Execution time**  = $\Big[\frac{C_{pp} * C_{ct}}{C_r}\Big]$, $C_{pp}$ = Cycles per program, $C_{ct}$ = Clock cycle time, ${C_r}$ = clock rate
We decrease execution time by either increasing clock rate or decreasing the number of clock cycles.

**Performance** For a machine $A$ running a program $P$ (where higher is faster):
$Perf(A,P) = \frac{1}{Time(A,P)}$  
$Perf(A,P) > Perf(B,P) \to Time(A,P) < Time(B, P)$  
$\Big[\frac{Perf(A,P)}{Perf(B,P)} = \frac{Time(B,P)}{Time(A,P)} = n\Big]$, where $A$ is $n$ times faster than B when $n > 1$.  
$Speedup = \frac{Time(old)}{Time(new)}$

**Little's Law** $\to Parellelism = Throughput * Latency$
:hugs: